{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import urllib.request\n",
    "import os, struct\n",
    "import numpy as np\n",
    "import gzip\n",
    "from array import array as pyarray\n",
    "from numpy import append, array, int8, uint8, zeros\n",
    "from collections import namedtuple\n",
    "from sklearn.preprocessing import normalize\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import LogisticRegression, Lasso\n",
    "from sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\n",
    "from sklearn.cross_validation import KFold"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def download_data(directory='./data/'):\n",
    "    \"\"\"Download MNIST database\"\"\"\n",
    "    urllib.request.urlretrieve(\"http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\",\n",
    "                               directory + \"/train-images-idx3-ubyte.gz\")\n",
    "    urllib.request.urlretrieve(\"http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\",\n",
    "                               directory + \"/train-labels-idx1-ubyte.gz\")\n",
    "    urllib.request.urlretrieve(\"http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\",\n",
    "                               directory + \"/t10k-images-idx3-ubyte.gz\")\n",
    "    urllib.request.urlretrieve(\"http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\",\n",
    "                               directory + \"/t10k-labels-idx1-ubyte.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def load_mnist(dataset=\"training\", digits=np.arange(10), path=\"./data/\"):\n",
    "    \"\"\"Loads MNIST files into 3D numpy arrays\n",
    "\n",
    "    Adapted from: http://abel.ee.ucla.edu/cvxopt/_downloads/mnist.py\n",
    "    \"\"\"\n",
    "\n",
    "    if dataset == \"training\":\n",
    "        fname_img = os.path.join(path, 'train-images-idx3-ubyte.gz')\n",
    "        fname_lbl = os.path.join(path, 'train-labels-idx1-ubyte.gz')\n",
    "    elif dataset == \"testing\":\n",
    "        fname_img = os.path.join(path, 't10k-images-idx3-ubyte.gz')\n",
    "        fname_lbl = os.path.join(path, 't10k-labels-idx1-ubyte.gz')\n",
    "    else:\n",
    "        raise ValueError(\"dataset must be 'testing' or 'training'\")\n",
    "\n",
    "    flbl = gzip.open(fname_lbl, 'rb')\n",
    "    magic_nr, size = struct.unpack(\">II\", flbl.read(8))\n",
    "    lbl = pyarray(\"b\", flbl.read())\n",
    "    flbl.close()\n",
    "\n",
    "    fimg = gzip.open(fname_img, 'rb')\n",
    "    magic_nr, size, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n",
    "    img = pyarray(\"B\", fimg.read())\n",
    "    fimg.close()\n",
    "\n",
    "    ind = [k for k in range(size) if lbl[k] in digits]\n",
    "    N = len(ind)\n",
    "\n",
    "    images = zeros((N, rows, cols), dtype=uint8)\n",
    "    labels = zeros((N, 1), dtype=int8)\n",
    "    for i in range(len(ind)):\n",
    "        images[i] = array(img[ ind[i]*rows*cols : (ind[i]+1)*rows*cols ]).reshape((rows, cols))\n",
    "        labels[i] = lbl[ind[i]]\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def split_data():\n",
    "    \"\"\"Preprocess and return training and testing datasets\"\"\"\n",
    "\n",
    "    images = namedtuple('MNIST_images', ['train', 'test'])\n",
    "    labels = namedtuple('MNIST_labels', ['train', 'test'])\n",
    "    images.train, labels.train = load_mnist('training')\n",
    "    images.test, labels.test = load_mnist('testing')\n",
    "\n",
    "    images.train = images.train.reshape(images.train.shape[0], -1)\n",
    "    images.test = images.test.reshape(images.test.shape[0], -1)\n",
    "\n",
    "    images.train = normalize(images.train.astype(np.float), axis=1)\n",
    "    images.test = normalize(images.test.astype(np.float), axis=1)\n",
    "\n",
    "    labels.train = labels.train.ravel()\n",
    "    labels.test = labels.test.ravel()\n",
    "\n",
    "    return images, labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "download_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "images, labels = split_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def reverse_splits(splits):\n",
    "    return [(split[1], split[0]) for split in splits]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class Stacking(object):    \n",
    "    \"\"\"Base class for stacking method of learning\"\"\"\n",
    "\n",
    "    def __init__(self, base_fitter, meta_fitter, \n",
    "                 split=lambda I: list(KFold(n=I.size, n_folds=2, shuffle=True)),\n",
    "                 decision_rule=lambda estimations: max(set(estimations), key=estimations.count)):\n",
    "        self.split = split\n",
    "        self.base_fitter = base_fitter\n",
    "        self.meta_fitter = meta_fitter\n",
    "        self.decision_rule = decision_rule  \n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Build compositions of classifiers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
    "         \n",
    "        y : array-like, shape = [n_samples]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        self : object\n",
    "            Returns self.\n",
    "        \"\"\"\n",
    "        self.classifiers = []\n",
    "        I = np.arange(y.size)\n",
    "        partitions = self.split(I)\n",
    "\n",
    "        for partition in partitions:\n",
    "            base_subsample, meta_subsample = partition\n",
    "\n",
    "            base_classifier = self.base_fitter(X[base_subsample], y[base_subsample])\n",
    "                    \n",
    "            X_meta = np.hstack((X[meta_subsample], \n",
    "                                base_classifier.predict(X[meta_subsample]).reshape(meta_subsample.size, -1)))\n",
    "            \n",
    "            meta_classifier = self.meta_fitter(X_meta, y[meta_subsample])\n",
    "            \n",
    "            self.classifiers.append(namedtuple('classifier', ['base', 'meta'])(base_classifier, meta_classifier))\n",
    "            \n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):      \n",
    "        \"\"\"Predict class for X.\n",
    "\n",
    "        The predicted class of an input sample is computed as the majority\n",
    "        prediction of the meta-classifiers.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        X : array-like or sparse matrix of shape = [n_samples, n_features]\n",
    "        \n",
    "        Returns\n",
    "        -------\n",
    "        y : array of shape = [n_samples]\n",
    "            The predicted classes.\n",
    "        \"\"\"\n",
    "        y_predicted = []\n",
    "       \n",
    "        estimations_matrix = np.array([], dtype=int).reshape(X.shape[0], 0)\n",
    "\n",
    "        for classifier in self.classifiers:\n",
    "            meta_features = classifier.base.predict(X)\n",
    "            estimations_matrix = np.column_stack((estimations_matrix, \n",
    "                                            classifier.meta.predict(np.column_stack((X, meta_features)))))\n",
    "            \n",
    "        y_predicted = [self.decision_rule(list(estimations)) for estimations in estimations_matrix]\n",
    "\n",
    "        return y_predicted     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Classifier(object):\n",
    "    \"\"\"Classifier wrapper\"\"\"\n",
    "    def __init__(self, predict_function):\n",
    "        self.predict = predict_function\n",
    "\n",
    "def get_SVM_fitter(C=1., kernel='linear', degree=3, gamma=1.):\n",
    "    return lambda X, y: Classifier(SVC(C=C, kernel=kernel, degree=degree, gamma=gamma).fit(X, y).predict)\n",
    "\n",
    "def SVM_fitter(X, y):\n",
    "    classifier = SVC(kernel=kernel, degree=degree, gamma=gamma).fit(X, y)\n",
    "    return Classifier(classifier.predict)\n",
    "\n",
    "def logit_fitter(X, y):\n",
    "    classifier = LogisticRegression('l2', False).fit(X, y)\n",
    "    return Classifier(classifier.predict)\n",
    "\n",
    "def random_forest_fitter(X, y):\n",
    "    classifier = RandomForestClassifier().fit(X, y)\n",
    "    return Classifier(classifier.predict)\n",
    "\n",
    "def random_forest_proba_fitter(X, y):\n",
    "    classifier = RandomForestClassifier().fit(X, y)\n",
    "    return Classifier(classifier.predict_proba)\n",
    "\n",
    "def extra_trees_proba_fitter(X, y):\n",
    "    classifier = ExtraTreesClassifier().fit(X, y)\n",
    "    return Classifier(classifier.predict_proba)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experiments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "pca = PCA(50).fit(images.train)\n",
    "pca_images_train = pca.transform(images.train)\n",
    "pca_images_test = pca.transform(images.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models = namedtuple('models', ['logit', 'svm', 'random_forest', 'extra_trees'])\n",
    "predictions = namedtuple('predictions', ['logit', 'svm', 'random_forest', 'extra_trees'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "models.logit = LogisticRegression('l2', False).fit(pca_images_train, labels.train)\n",
    "predictions.logit = models.logit.predict(pca_images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.098100000000000007"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predictions.logit != labels.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models.svm = SVC(kernel = 'linear').fit(pca_images_train, labels.train)\n",
    "predictions.svm = models.svm.predict(pca_images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.062399999999999997"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predictions.svm != labels.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models.random_forest = RandomForestClassifier().fit(pca_images_train, labels.train)\n",
    "predictions.random_forest = models.random_forest.predict(pca_images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.069000000000000006"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predictions.random_forest != labels.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra trees classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "models.extra_trees = ExtraTreesClassifier().fit(pca_images_train, labels.train)\n",
    "predictions.extra_trees = models.extra_trees.predict(pca_images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.071999999999999995"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(predictions.extra_trees != labels.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Stacking classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_svm = Stacking(base_fitter=logit_fitter, meta_fitter=get_SVM_fitter(C=5, kernel='poly', degree = 2), \n",
    "                     split=lambda I: list(KFold(n=I.size, n_folds=5, shuffle=True)))\n",
    "logit_svm_predictions = logit_svm.fit(pca_images_train, labels.train).predict(pca_images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0378"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(logit_svm_predictions != labels.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression + Random forest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_rf = Stacking(base_fitter=logit_fitter, meta_fitter=random_forest_fitter, \n",
    "                    split=lambda I: list(KFold(n=I.size, n_folds=5, shuffle=True)))\n",
    "logit_rf_predictions = logit_rf.fit(pca_images_train, labels.train).predict(pca_images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0746"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(logit_rf_predictions != labels.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra trees + Logistic regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "et_logit = Stacking(base_fitter=extra_trees_proba_fitter, meta_fitter=logit_fitter, \n",
    "                    split=lambda I: list(KFold(n=I.size, n_folds=5, shuffle=True)))\n",
    "et_logit_predictions = et_logit.fit(pca_images_train, labels.train).predict(pca_images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.041700000000000001"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(et_logit_predictions != labels.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "svm_svm = Stacking(base_fitter=get_SVM_fitter(), meta_fitter=get_SVM_fitter(C=5, kernel='poly', degree = 2), \n",
    "                   split=lambda I: list(KFold(n=I.size, n_folds=5, shuffle=True)))\n",
    "svm_svm_predictions = svm_svm.fit(pca_images_train, labels.train).predict(pca_images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.038399999999999997"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(svm_svm_predictions != labels.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random forest + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "rf_svm = Stacking(base_fitter=random_forest_fitter, meta_fitter=get_SVM_fitter(C=10, kernel='poly', degree = 2), \n",
    "                  split=lambda I: list(KFold(n=I.size, n_folds=5, shuffle=True)))\n",
    "rf_svm_predictions = rf_svm.fit(pca_images_train, labels.train).predict(pca_images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0339"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(rf_svm_predictions != labels.test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extra trees + SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "wildfowl = Stacking(base_fitter=extra_trees_proba_fitter, meta_fitter=get_SVM_fitter(C=5, kernel='poly', degree = 2), \n",
    "                    split=lambda I: list(KFold(n=I.size, n_folds=5, shuffle=True)))\n",
    "wildfowl_predictions = wildfowl.fit(pca_images_train, labels.train).predict(pca_images_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.024199999999999999"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.mean(wildfowl_predictions != labels.test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0237\n",
      "0.0215\n",
      "0.0249\n",
      "0.0269\n",
      "0.0275\n",
      "0.0282\n"
     ]
    }
   ],
   "source": [
    "for n_folds in [2, 3, 5, 10, 15, 20]:   \n",
    "    et_logit = Stacking(base_fitter=extra_trees_proba_fitter, meta_fitter=get_SVM_fitter(C=5, kernel='poly', degree = 2), \n",
    "                        split=lambda I: list(KFold(n=I.size, n_folds=n_folds, shuffle=True)))\n",
    "    et_logit_predictions = et_logit.fit(pca_images_train, labels.train).predict(pca_images_test)\n",
    "    print(np.mean(et_logit_predictions != labels.test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
